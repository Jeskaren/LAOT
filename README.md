# LAOT: A Benchmark for Low-Light and Amodal Occlusion Tracking

[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)
[![Paper](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org/abs/xxxx.xxxxx)

**LAOT** (Low-light and Amodal Occlusion Tracking) is a challenging benchmark designed to evaluate object tracking algorithms under real-world visual degradation — specifically **low-light conditions** and **partial/full occlusion**. It is the first benchmark to combine both challenges in a standardized dataset and evaluation framework.

---

## 🚀 Highlights

- 🌙 **Low-Light Tracking**: Real-world sequences captured in dark environments with varying illumination.
- 🧱 **Amodal Occlusion Reasoning**: Frame-level occlusion annotations (including occlusion ratios).
- 🧪 **Fine-Grained Evaluation**: AP metrics under three occlusion levels: *No Occlusion*, *Partial Occlusion*, *Heavy Occlusion*.
- 🛠️ **Flexible Evaluation Code**: Python-based evaluation tools with IOU and occlusion analysis support.
- 📊 **21 Popular Trackers Evaluated**: Includes deep learning-based and traditional methods.

---

## 📁 Dataset Structure

