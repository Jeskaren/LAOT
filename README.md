# LAOT: A Benchmark for Low-Light and Amodal Occlusion Tracking

[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)
[![Paper](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org/abs/xxxx.xxxxx)

**LAOT** (Low-light and Amodal Occlusion Tracking) is a challenging benchmark designed to evaluate object tracking algorithms under real-world visual degradation â€” specifically **low-light conditions** and **partial/full occlusion**. It is the first benchmark to combine both challenges in a standardized dataset and evaluation framework.

---

## ğŸš€ Highlights

- ğŸŒ™ **Low-Light Tracking**: Real-world sequences captured in dark environments with varying illumination.
- ğŸ§± **Amodal Occlusion Reasoning**: Frame-level occlusion annotations (including occlusion ratios).
- ğŸ§ª **Fine-Grained Evaluation**: AP metrics under three occlusion levels: *No Occlusion*, *Partial Occlusion*, *Heavy Occlusion*.
- ğŸ› ï¸ **Flexible Evaluation Code**: Python-based evaluation tools with IOU and occlusion analysis support.
- ğŸ“Š **21 Popular Trackers Evaluated**: Includes deep learning-based and traditional methods.

---

## ğŸ“ Dataset Structure

